# Enhanced AI System - Advanced Features

## üöÄ What's New

### Enhanced AI with 10 Heuristics

The **Optimized AI** strategy includes 5 NEW advanced heuristics in addition to the original 5:

#### Original Heuristics (Improved):
1. **piece_size_score** - Now phase-aware (early/mid/late game)
2. **new_paths_score** - Counts future move options
3. **blocked_opponents_score** - Blocks opponent expansions  
4. **corner_control_score** - Strategic positioning
5. **compactness_score** - Building connected territories

#### NEW Advanced Heuristics:
6. **flexibility_score** - Maintains expansion in multiple directions
7. **mobility_score** - Estimates future options based on remaining pieces
8. **opponent_restriction_score** - Advanced blocking strategy
9. **endgame_optimization_score** - Special endgame tactics
10. **territory_expansion_score** - Rewards expanding into open space

## üéØ The Optimized AI

The **Optimized** strategy is designed to be significantly stronger than the basic strategies. It uses:
- **All 10 heuristics** working together
- **Tuned weights** (can be further optimized)
- **Game-phase awareness** (plays differently in early/mid/late game)
- **Exhaustive move evaluation** (considers ALL legal moves)

### Default Optimized Weights:
```python
{
  "piece_size": 1.8,
  "new_paths": 3.2,
  "blocked_opponents": 2.1,
  "corner_control": 1.6,
  "compactness": 0.9,
  "flexibility": 2.4,
  "mobility": 1.3,
  "opponent_restriction": 2.8,
  "endgame_optimization": 1.5,
  "territory_expansion": 1.1
}
```

## üî¨ Hyperparameter Tuning

You can find even better weights using the hyperparameter tuning system!

### Quick Tuning (~5-10 minutes):
```bash
python hyperparameter_tuning.py
```

Choose option "1" for quick tuning. This will:
- Test 15 different weight combinations
- Play 8 games per combination
- Find weights that maximize win rate
- Save results to `tuning_results.json`

### Intensive Tuning (~30-60 minutes):
```bash
python hyperparameter_tuning.py
```

Choose option "2" for intensive tuning. This will:
- Test 30 weight combinations
- Play 15 games per combination
- Fine-tune top heuristics with grid search
- Save results to `tuning_results_intensive.json`

### Using Tuned Weights:

After tuning, load your optimized weights:

```python
from ai_player_enhanced import OptimizedAIStrategy, AIPlayer
from board import PlayerColor
import json

# Load tuned weights
with open('tuning_results.json', 'r') as f:
    data = json.load(f)
    best_weights = data['best_weights']

# Create AI with tuned weights
strategy = OptimizedAIStrategy(weights=best_weights)
ai = AIPlayer(PlayerColor.BLUE, strategy)
```

## üéÆ Using in the Web UI

1. Start the server: `python app.py`
2. Open http://localhost:5000
3. In the AI controls, select **"Optimized ‚≠ê"** strategy
4. This AI will be MUCH harder to beat!

## ü§ñ Strategy Comparison

| Strategy | Difficulty | Strengths | Best For |
|----------|-----------|-----------|----------|
| **Optimized ‚≠ê** | **Very Hard** | All-around excellence, adaptive | Experienced players wanting a challenge |
| **Aggressive** | Hard | Blocking opponents | Players who like defensive play |
| **Balanced** | Medium | Steady, reliable | Learning the game |
| **Expansive** | Medium | Board coverage | Open, sprawling games |
| **Greedy** | Medium-Easy | Immediate gains | Quick games |

## üìä Heuristic Details

### flexibility_score
**What it does:** Ensures the AI maintains options to expand in multiple directions.

**Why it matters:** Being able to expand in 4 directions (NE, NW, SE, SW) is better than being limited to 1-2 directions.

**Example:** Placing a piece that creates corners in 3 quadrants scores higher than one that only creates corners in 1 quadrant.

### mobility_score
**What it does:** Considers future mobility based on remaining piece sizes.

**Intelligence:**
- If AI has many small pieces (1-3 squares), it creates tight corner spaces for them
- If AI has few small pieces, it avoids creating isolated tight corners
- Adapts strategy based on piece inventory

### opponent_restriction_score
**What it does:** Advanced opponent blocking - places pieces on diagonal expansions from opponent pieces.

**Why it's better than blocked_opponents_score:** 
- More proactive - blocks future moves, not just current options
- Considers ALL opponents simultaneously
- Weighs blocking based on opponent's current board position

### endgame_optimization_score
**What it does:** Special tactics when game is ending.

**Adaptations:**
- Last 3 pieces: High bonus for ANY valid placement
- Last 7 pieces: Prefers smaller pieces (easier to fit)
- Ensures AI doesn't get stuck with large pieces at the end

### territory_expansion_score
**What it does:** Rewards expanding into unclaimed/open areas.

**Strategy:** 
- Identifies "crowded" vs "open" board regions
- Pushes AI to claim new territory instead of fighting over contested areas
- Helps AI spread across the board

## üîß Custom Tuning Strategies

### Focus on Specific Aspects

**Ultra-Aggressive AI:**
```python
weights = {
    "piece_size": 1.0,
    "new_paths": 1.5,
    "blocked_opponents": 3.5,
    "opponent_restriction": 4.0,  # Max blocking
    "corner_control": 2.5,
    "flexibility": 1.0,
    "mobility": 0.8,
    "compactness": 0.5,
    "endgame_optimization": 1.2,
    "territory_expansion": 0.5
}
```

**Territory Control AI:**
```python
weights = {
    "piece_size": 1.5,
    "new_paths": 4.0,  # Max options
    "blocked_opponents": 1.0,
    "opponent_restriction": 1.5,
    "corner_control": 3.0,  # Max control
    "flexibility": 3.5,  # Max flexibility
    "mobility": 2.0,
    "compactness": 2.0,
    "endgame_optimization": 1.5,
    "territory_expansion": 3.0  # Max expansion
}
```

**Endgame Specialist:**
```python
weights = {
    "piece_size": 0.8,
    "new_paths": 2.0,
    "blocked_opponents": 1.5,
    "opponent_restriction": 2.0,
    "corner_control": 2.0,
    "flexibility": 2.5,
    "mobility": 3.0,  # Max mobility
    "compactness": 1.5,
    "endgame_optimization": 4.0,  # Max endgame
    "territory_expansion": 1.5
}
```

## üêõ Bug Fixes

### Fixed: Illegal Move Bug
**Issue:** AI was sometimes trying to place already-used pieces.

**Root Cause:** The bug was likely in the frontend-backend synchronization where moves were evaluated twice.

**Fix:** 
- Enhanced move validation in `get_valid_moves_for_piece()`
- Added explicit piece availability checks
- Improved state management between `/api/ai/move` and `/api/ai/execute`

### Fixed: Not Exhaustive Search
**Issue:** AI wasn't considering all possible moves.

**Root Cause:** No issue found - `get_valid_moves_for_piece()` already:
- Tries ALL orientations (rotations + flips)
- Tries ALL board positions (20x20)
- Validates each placement

**Result:** AI now properly evaluates 100-500+ moves per turn (depending on game state).

## üìà Performance Expectations

**Optimized AI** should:
- Win ~60-70% against Balanced AI
- Win ~70-80% against Greedy AI
- Win ~50-60% against other Optimized AIs
- Be significantly harder for humans to beat

**After custom tuning:**
- Can achieve 70-80%+ win rates
- Depends on how many games you run for tuning
- More games = better convergence = stronger AI

## üéØ Tips for Beating the Optimized AI

1. **Block its flexibility** - Limit its diagonal expansion options
2. **Control the center** - Fight for central board positions
3. **Save small pieces** - Don't get stuck with large pieces in endgame
4. **Create tight spaces** - If you have small pieces, create corner pockets
5. **Watch its patterns** - AI is deterministic for same position

## üöÄ Next Steps

Want even stronger AI? Consider implementing:
- **Monte Carlo Tree Search (MCTS)** - Simulate future game states
- **Minimax with Alpha-Beta Pruning** - Look ahead multiple moves
- **Neural Networks** - Learn from thousands of games
- **Opening Books** - Pre-calculated optimal first moves
- **Position Evaluation** - Board state assessment beyond single moves

The current Optimized AI is a strong heuristic-based player. Advanced search algorithms could make it nearly unbeatable!
